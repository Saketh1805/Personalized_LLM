{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9043e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llamaapi as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"24 folders each has 56 sheets of paper inside them. How many sheets of paper are there altogether?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ea5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7530715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21da8bf6b5ec486bb3b154920dbcba4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama2_7 = L.LLM(\"llama2\", \"7b\")\n",
    "\n",
    "#llama2_70 = L.LLM(\"llama2\", \"70b\")\n",
    "\n",
    "#vicuna_13 = L.LLM(\"vicuna\", \"13b\")\n",
    "\n",
    "#falcon_7 = L.LLM(\"falcon\", \"7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vicuna_7 = L.LLM(\"vicuna\", \"7b\")\n",
    "#vicuna_13.ask(question)\n",
    "#falcon_7.ask(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e4f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiran.sandilya001/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie Jailer is a Tamil-language action-thriller film directed by Nelson Dilipkumar and starring Rajinikanth in the lead role. The film was released on 29th July 2023 and received mixed reviews from critics. While some praised Rajinikanth's performance, others felt that the film lacked originality and had too much violence. The film's box office collections were also average, with a worldwide total of 600 crore rupees. There were also reports of a potential sequel to the film. (Source: [63][64][65][66][67][68][69][70][71][72])\n",
      "who is director of jailer movie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The director of the movie \"Jailer\" is Nelson Dilipkumar.\n",
      "who is heroine of jailer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heroine of Jailer is Priyanka Chopra.\n",
      "who is raginikanth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rajinikanth is the answer to the query.\n",
      "who is the lead role in jailer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rajinikanth is the lead role in Jailer.\n",
      "who is nelson dilipkumar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nelson Dilipkumar is a film director and screenwriter known for his work in Tamil cinema. He is the director of the upcoming film \"Jailer,\" which stars Rajinikanth and Shivarajkumar. Dilipkumar has also worked as an assistant director to filmmaker K. Bhagyaraj.\n"
     ]
    }
   ],
   "source": [
    "L.finetune(llama2_7, \"testdir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.comparator(question,vicuna_7,vicuna_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd239808",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.comparator(question,falcon_7, llama2_7, vicuna_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f4f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = L.LLM(\"vicuna\", \"13b\")\n",
    "a.ask(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9760d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83231729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = L.LLM(\"llama2\", \"7b\")\n",
    "\n",
    "\n",
    "b.ask(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "c = L.LLM(\"llama2\", \"13b\")\n",
    "\n",
    "c.ask(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = L.LLM(\"falcon\", \"7b\")\n",
    "\n",
    "\n",
    "d.ask(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4702888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3979d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdd25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"llama2/7b\",\n",
    "    model_name=\"llama2/7b\",\n",
    "    device_map=\"auto\",\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "\n",
    "\n",
    "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
    "\n",
    "\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbf450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Tell me about the movie Jailer\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "while True:\n",
    "  query=input()\n",
    "  response = query_engine.query(query)\n",
    "  print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
